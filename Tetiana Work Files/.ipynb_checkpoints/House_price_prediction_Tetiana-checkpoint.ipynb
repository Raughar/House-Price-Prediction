{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55226a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f0b7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlrd in c:\\users\\tetiana\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4df311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_excel(r\"C:\\Users\\Tetiana\\Documents\\ironhack\\lab\\House-Price-Prediction\\regression_data.xls\")\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46635c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a30795",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08d072e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03be76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2a8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e656f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numerical = df.select_dtypes('number')\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d232b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in numerical.columns:\n",
    "    sns.displot(numerical[val])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c7caa0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fba9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d190ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in numerical.columns:\n",
    "    print(numerical[column].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0742b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = abs(np.quantile(df[\"price\"], .25) - np.quantile(df[\"price\"], .75)) * 1.5\n",
    "lower_boundary = np.quantile(df[\"price\"], .25) - IQR\n",
    "upper_boundary = np.quantile(df[\"price\"], .75) + IQR\n",
    "lower_boundary, upper_boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009a4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"price\"] >1129500.0) | (df[\"price\"] < -162500.0)].sort_values(\"price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71c789",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_indices = df[(df[\"price\"] > 1129500.0) | (df[\"price\"] < -162500.0)].index\n",
    "df = df.drop(outliers_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe901d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.select_dtypes('number'):\n",
    "    sns.barplot(x=df[col], y=df['price'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe65fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df['neighborhoods'] = df['zipcode'].apply(lambda x: str(x)[:4])\n",
    "\n",
    "#Checking the unique values of the column to see if the grouping was successful\n",
    "df['neighborhoods'].unique()\n",
    "\n",
    "#Changing the value names of the neighborhoods to make it easier to understand using only the last two digits\n",
    "for val in df['neighborhoods'].unique():\n",
    "    df['neighborhoods'] = df['neighborhoods'].replace(val, str(val)[2:])\n",
    "\n",
    "#Converting the column to int\n",
    "df['neighborhoods'] = df['neighborhoods'].astype(int)\n",
    "df['neighborhoods'].value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae46ca04",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def categorize_zipcodes(latitudes, longitudes):\n",
    "    city_center_lat = 47.6062  # Latitude of downtown Seattle\n",
    "    city_center_long = -122.3321  # Longitude of downtown Seattle\n",
    "    \n",
    "    # Define the boundaries for city center and suburb\n",
    "    city_center_range = 0.1  # Adjust this value based on the desired radius around downtown\n",
    "    suburb_range = 0.5  # Adjust this value based on the desired distance from downtown\n",
    "    \n",
    "    categories = []\n",
    "    for lat, long in zip(latitudes, longitudes):\n",
    "        # Calculate distances from downtown Seattle\n",
    "        distance_from_center = ((lat - city_center_lat) ** 2 + (long - city_center_long) ** 2) ** 0.5\n",
    "\n",
    "        if distance_from_center <= city_center_range:\n",
    "            categories.append('City Center')\n",
    "        elif distance_from_center <= suburb_range:\n",
    "            categories.append('Suburb')\n",
    "        else:\n",
    "            categories.append('Outskirts')\n",
    "    \n",
    "    return categories\n",
    "\n",
    "# Apply the categorization function to the latitude and longitude columns\n",
    "zipcodes_categories = categorize_zipcodes(df['lat'], df['long'])\n",
    "df['zipcodes_categories'] = zipcodes_categories\n",
    "df['zipcodes_categories'].value_counts()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9dad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c449d1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(df['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['id','date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b6f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511ce8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing the house price data\n",
    "# Convert 'zipcode' column to categorical data type\n",
    "\n",
    "# Perform one-hot encoding\n",
    "df = pd.get_dummies(df, columns=['zipcodes_categories'], drop_first=True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1d6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_matrix = df.corr()\n",
    "correlations_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8216113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_matrix[\"price\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271190f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros_like(correlations_matrix)\n",
    "mask[np.triu_indices_from(mask)] = True \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ax = sns.heatmap(correlations_matrix, mask=mask, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7e2449",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    print(col)\n",
    "    sns.boxplot(numerical[col])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'df' is your DataFrame with dummy encoded columns\n",
    "# Apply log scaling to the entire dataset\n",
    "data_log_scaled = df.applymap(lambda x: np.log(x) if isinstance(x, (int, float)) and x > 0 else x)\n",
    "\n",
    "# Print the log-scaled dataset\n",
    "print(data_log_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_log_scaled['price']\n",
    "X = data_log_scaled.drop(columns='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f23a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b735da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_scaled['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b31d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data_log_scaled['price'].value_counts()\n",
    "count_of_values_with_count_1 = 0\n",
    "for val, count in counts.items():\n",
    "    if count == 1:\n",
    "        count_of_values_with_count_1 += 1\n",
    "print(count_of_values_with_count_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fece89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the log-scaled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Train the linear regression model\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_std = lr.predict(X_test_std)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"R2:\", lr.score(X_test_std, y_test))\n",
    "print(\"MAE:\", mean_absolute_error(predictions_std, y_test))\n",
    "print(\"RMSE:\", mean_squared_error(predictions_std, y_test, squared=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4813909a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_importance = pd.Series([abs(i) for i in lr.coef_], index=X_train.columns).sort_values(ascending=False)\n",
    "data_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5da210",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_log_scaled[data_importance[:17].index]  # data_final_outliers\n",
    "y = data_log_scaled[\"price\"]  # data_final_outliers\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "predictions = lr.predict(X_test_std)\n",
    "print(\"R2:\", lr.score(X_test_std, y_test))\n",
    "print(\"MAE:\", mean_absolute_error(predictions, y_test))\n",
    "print(\"RMSE:\", mean_squared_error(predictions, y_test, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_scaled.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae63ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_log_scaled.drop(columns=['sqft_living'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f82fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# vif uses OLS to calculate the factor, so we don't have the intercept\n",
    "# that's why we need to assign it:\n",
    "vif = add_constant(data_log_scaled.drop(columns=['price']))\n",
    "\n",
    "# To remove multicolinearity automatically:\n",
    "threshold = 10\n",
    "\n",
    "while True:\n",
    "    # calculates vif\n",
    "    values = [variance_inflation_factor(np.array(vif), i)\n",
    "              for i in range(len(vif.columns))][1:]\n",
    "    display(pd.DataFrame(values, index=vif.columns[1:]).sort_values(0))\n",
    "    # checks if the highest vif is bigger than the threshold\n",
    "    if max(values) > threshold:\n",
    "        # finds the column that has the maximum vif\n",
    "        col_index = values.index(max(values)) + 1\n",
    "        column_name = vif.columns[col_index]\n",
    "        # drops that column\n",
    "        vif = vif.drop(column_name, axis=1)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f20671",
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in data_log_scaled.columns if col not in vif]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ccbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually, just to see what the code is doing:\n",
    "\n",
    "vif_ = pd.DataFrame(index=vif.columns)\n",
    "\n",
    "vif_[\"VIF Factor\"] = [round(variance_inflation_factor(np.array(vif), i),2)\n",
    "                     for i in range(len(vif.columns))]\n",
    "\n",
    "vif_.sort_values(\"VIF Factor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069e2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector, respectively\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "rf_regressor = RandomForestRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf_regressor.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = rf_regressor.predict(X_test_std)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05cd4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the residuals\n",
    "predicted_values = rf_regressor.predict(X_test_std)\n",
    "residuals = y_test - predicted_values\n",
    "# Create a residual plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=predicted_values, y=residuals)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Residual Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22d80a0",
   "metadata": {},
   "source": [
    "In this residual plot, the points are scattered randomly around the residual=0 line. We can conclude that a linear model is appropriate for modeling this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a histogram of the residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title('Histogram of Residuals')\n",
    "plt.xlabel('Residuals')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f4c79e",
   "metadata": {},
   "source": [
    "We can see normaly destributed residuals - it means that our model works good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96db9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'residuals' is your array of residuals\n",
    "plt.figure(figsize=(8, 6))\n",
    "stats.probplot(residuals, plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49eeec",
   "metadata": {},
   "source": [
    "Our data close to 45 degree line - we can conclude that our data normaly destributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the residuals vs the predicted values\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=predicted_values, y=np.abs(residuals))\n",
    "plt.title('Residuals vs. Predicted Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Absolute Residuals')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbb0913",
   "metadata": {},
   "source": [
    "We can see random destribution of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dae9e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different values for n_estimators\n",
    "n_estimators_list = [50, 100, 150, 200]\n",
    "\n",
    "for n_estimators in n_estimators_list:\n",
    "    rf_regressor = RandomForestRegressor(n_estimators=n_estimators, random_state=42)\n",
    "    rf_regressor.fit(X_train, y_train)\n",
    "    predictions = rf_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    print(\"n_estimators:\", n_estimators)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"Mean Absolute Error:\", mae)\n",
    "    print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create ExtraTreesRegressor model\n",
    "extra_trees_model = ExtraTreesRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "extra_trees_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = extra_trees_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector, respectively\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting Regressor model\n",
    "gbm = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gbm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = gbm.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "r2 = r2_score(y_test, predictions)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d3197c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'X' is your feature matrix and 'y' is your target variable\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create AdaBoostRegressor model\n",
    "ada_boost_model = AdaBoostRegressor(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "ada_boost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = ada_boost_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d26d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector, respectively\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Ridge Regression model\n",
    "ridge = Ridge(alpha=1.0)  # You can adjust the regularization strength using the alpha parameter\n",
    "\n",
    "# Fit the model to the training data\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = ridge.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"R2:\", ridge.score(X_test_std, y_test))\n",
    "print(\"MAE:\", mean_absolute_error(predictions_std, y_test))\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Assuming X and y are your feature matrix and target vector, respectively\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Create a Lasso Regression model\n",
    "lasso = Lasso(alpha=1.0)  # You can adjust the regularization strength using the alpha parameter\n",
    "\n",
    "# Fit the model to the training data\n",
    "lasso.fit(X_train_std, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lasso.predict(X_test_std)\n",
    "\n",
    "# Calculate Mean Squared Error\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(\"R2:\", lasso.score(X_test_std, y_test))\n",
    "print(\"MAE:\", mean_absolute_error(predictions, y_test))\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
